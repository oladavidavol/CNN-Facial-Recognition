{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf4dd54d",
   "metadata": {},
   "source": [
    "## 1. Facial Detection and Image Capturing for Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb59037b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b4ff00e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"*******************************************************\\n\")     \n",
    "print(\"Welcome to the Facial Access Control Center\\n\")\n",
    "print(\"This program will register you on the database and be used to grant you access to restricted areas\\n\")\n",
    "print(\"Your facial records are kept securely on this database\\nData collected contains no identifying information and isn't shared with third parties\\n\")\n",
    "print(\"*******************************************************\\n\")\n",
    "\n",
    "consent_answer = input(\"Press q and enter if you do not consent and wish to quit\\nPress any other key and enter to continue ===>  \")\n",
    "\n",
    "import sys\n",
    "if consent_answer == (\"q\") or consent_answer == (\"Q\"):\n",
    "    print(\"Thanks for your response, good bye\\n\")\n",
    "    quit()\n",
    "else:\n",
    "    print(\"Welcome to the program\")\n",
    "    \n",
    "import cv2\n",
    "import numpy as np\n",
    "count = 0\n",
    "import os\n",
    "\n",
    "#Setting up dnn\n",
    "modelFile = \"dnn_config/res10_300x300_ssd_iter_140000.caffemodel\"\n",
    "configFile = \"dnn_config/deploy.prototxt.txt\"\n",
    "net = cv2.dnn.readNetFromCaffe(configFile, modelFile)\n",
    "\n",
    "#Setting up the webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "#Font for the caption\n",
    "font = cv2.FONT_HERSHEY_SIMPLEX \n",
    "# For each person, enter a face id\n",
    "face_id = input('\\n enter user id end press <return> ==>  ')\n",
    "\n",
    "#Creates a working directory if it doesn't exist\n",
    "os.makedirs(\"headshots\", exist_ok=True)\n",
    "\n",
    "# Gets current working directory\n",
    "path = \"headshots\"\n",
    "\n",
    "# Joins the folder that we wanted to create\n",
    "folder_name = (face_id)\n",
    "path = os.path.join(path, folder_name) \n",
    "\n",
    "# Creates the folder, and checks if it is created or not.\n",
    "os.makedirs(path, exist_ok=True)\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\n [INFO] Initializing face capture. Look the camera and wait ...\")\n",
    "while(True):\n",
    "    ret, img = cap.read()\n",
    "    img = cv2.flip(img, 1)\n",
    "    \n",
    "    if ret == True:\n",
    "        img = cv2.resize(img, None, fx=0.8, fy=0.8)\n",
    "        height, width = img.shape[:2]\n",
    "        \n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        blob = cv2.dnn.blobFromImage(cv2.resize(img, (300, 300)),\n",
    "                                        1.0, (300, 300), (104.0, 117.0, 123.0))\n",
    "        net.setInput(blob)\n",
    "        faces3 = net.forward()\n",
    "        \n",
    "        for i in range(faces3.shape[2]):\n",
    "            confidence = faces3[0, 0, i, 2]\n",
    "            if confidence > 0.5:\n",
    "                box = faces3[0, 0, i, 3:7] * np.array([width, height, width, height])\n",
    "                (x, y, x1, y1) = box.astype(\"int\")\n",
    "                cv2.rectangle(img, (x, y), (x1, y1), (0, 0, 255), 2)\n",
    "                count += 1\n",
    "            \n",
    "        # Save the captured image into the datasets folder\n",
    "            cv2.imwrite(os.path.join(path, str(face_id) + '.' +  \n",
    "                    str(count) + \".jpg\"), gray[y:y1,x:x1])\n",
    "            \n",
    "        k = cv2.waitKey(100) & 0xff # Press 'ESC' for exiting video\n",
    "        if k == 27:\n",
    "            break\n",
    "        elif count >= 60: # Take 60 face sample and stop video\n",
    "            break\n",
    "        cv2.putText(img, 'Facial Detector', (30, 30), font, 1, (255, 255, 0), 2, cv2.LINE_AA)\n",
    "        \n",
    "        #######################\n",
    "\n",
    "        cv2.imshow(\"dnn\", img)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "    else:\n",
    "        break\n",
    "\n",
    "        \n",
    "\n",
    "           \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d0f64a",
   "metadata": {},
   "source": [
    "## 2. Image Augmentation and Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d809a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow.keras as keras\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dense, Conv2D, MaxPool2D , Flatten\n",
    "\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.mobilenet import preprocess_input\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67350d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluation imports\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e2c4e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#augmentation of training images, best used when the dataset is small\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1/255.,              # normalize pixel values between 0-1\n",
    "    brightness_range=[0.1, 0.7], # specify the range in which to decrease/increase brightness\n",
    "    width_shift_range=0.5,       # shift the width of the image 50%\n",
    "    rotation_range=90,           # random rotation by 90 degrees\n",
    "    horizontal_flip=True,        # 180 degree flip horizontally\n",
    "    vertical_flip=True,          # 180 degree flip vertically\n",
    "    validation_split=0.2        # 20% of the data will be used for validation at end of each epoch\n",
    ")\n",
    "\n",
    "#This is used to count how many classes of images you have\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "'./Headshots',\n",
    "target_size=(224,224),\n",
    "color_mode='rgb',\n",
    "batch_size=32,\n",
    "class_mode='categorical',\n",
    "shuffle=False, subset = 'training', seed=42)\n",
    "\n",
    "validation_generator = train_datagen.flow_from_directory(\n",
    "'./Headshots',\n",
    "target_size=(224,224),\n",
    "color_mode='rgb',\n",
    "batch_size=32,\n",
    "class_mode='categorical',\n",
    "shuffle=False, subset = 'validation', seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49308d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator.class_indices.values()\n",
    "# dict_values([0, 1, 2])\n",
    "NO_CLASSES = len(train_generator.class_indices.values())\n",
    "NO_CLASSES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7218b60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator.class_indices.keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17dc2f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator.class_indices.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01123dd3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras_vggface.vggface import VGGFace\n",
    "pbase_model = VGGFace(include_top=False,\n",
    "weights= 'vggface', #VGGFace is lightweight and thus can be transmitted across the network\n",
    "input_shape=(224, 224, 3))\n",
    "pbase_model.trainable = False\n",
    "pbase_model.summary()\n",
    "print(len(pbase_model.layers))\n",
    "# 19 layers after excluding the last few layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82384934",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from tensorflow.keras import layers, models\n",
    "\n",
    "flatten_layer = layers.Flatten()\n",
    "dense_layer_1 = layers.Dense(1024, activation='relu')\n",
    "dense_layer_2 = layers.Dense(1024, activation='relu')\n",
    "dense_layer_3 = layers.Dense(512, activation='relu')\n",
    "prediction_layer = layers.Dense(NO_CLASSES, activation='softmax')\n",
    "\n",
    "\n",
    "#Coupling both the top and the bottom layers\n",
    "pmodel = models.Sequential([\n",
    "    pbase_model,\n",
    "    flatten_layer,\n",
    "    dense_layer_1,\n",
    "    dense_layer_2,\n",
    "    dense_layer_3,\n",
    "    prediction_layer\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e1d61c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pmodel.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy'],\n",
    ")\n",
    "\n",
    "\n",
    "es = EarlyStopping(monitor='accuracy', mode='max', patience=7)\n",
    "\n",
    "history = pmodel.fit(train_generator, epochs=50, batch_size=32, verbose = 1,  validation_data=validation_generator, callbacks=[es])\n",
    "#validation_steps=10\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a21b452c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss, train_acc = pmodel.evaluate(train_generator, verbose=2)\n",
    "print('\\n Training accuracy:', train_acc)\n",
    "print('\\n Training loss:', train_loss)\n",
    "test_loss, test_acc = pmodel.evaluate(validation_generator, verbose=2)\n",
    "print('\\n Test accuracy:', test_acc)\n",
    "print('\\n Test loss:', test_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2750d8a",
   "metadata": {},
   "source": [
    "## Measurements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17381d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list all data in history\n",
    "print(history.history.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a33b3df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting historic Test and Training Accuracy\n",
    "acc_train = history.history['accuracy']\n",
    "acc_val = history.history['val_accuracy']\n",
    "epochs = range(1,38)\n",
    "plt.subplots(figsize = (10,10))\n",
    "plt.plot(epochs, acc_train, 'g', label='Historic Training Accuracy')\n",
    "plt.plot(epochs, acc_val, 'r', label='Historic Validation Accuracy')\n",
    "# Add a horizontal line (Test Accuracy, Valication Accuracy)\n",
    "plt.axhline(test_acc, linestyle = \"--\", color = 'y' ,label='Test accuracy')\n",
    "plt.axhline(train_acc, linestyle = \"--\", label='Train accuracy')\n",
    "plt.title('Training accuracy CNN')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.savefig(\"plot_acc3.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3048f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting Test and Training loss\n",
    "loss_train = history.history['loss']\n",
    "loss_val = history.history['val_loss']\n",
    "epochs = range(1,38)\n",
    "plt.subplots(figsize = (10,10))\n",
    "plt.plot(epochs, loss_train, 'g', label='Historic Training Loss')\n",
    "plt.plot(epochs, loss_val, 'r', label='Historic Validation Loss')\n",
    "# Add a horizontal line (Test Loss, training Loss)\n",
    "plt.axhline(test_loss, linestyle = \"--\", color = 'y', label='Test Loss')\n",
    "plt.axhline(train_loss, linestyle = \"--\", label='Training Loss')\n",
    "plt.title('Training Loss CNN')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.savefig(\"plot_loss3.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f1e999",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting Prediction Labels\n",
    "y_pred = pmodel.predict(validation_generator)\n",
    "y_pred_labels = np.argmax(y_pred, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b624b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Classification Report\n",
    "\n",
    "class_report = metrics.classification_report(validation_generator.classes, y_pred_labels, target_names=train_generator.class_indices.keys(),zero_division=1,  output_dict=False)\n",
    "print(class_report)    \n",
    "class_report1 = classification_report(validation_generator.classes, y_pred_labels, target_names=train_generator.class_indices.keys(),zero_division=1, output_dict=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8db81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "fig, ax = plt.subplots(figsize=(15,10)) \n",
    "plt.rcParams['font.size'] = '20'\n",
    "sns.heatmap(pd.DataFrame(class_report1).iloc[:-1, :].T, annot=True,)\n",
    "plt.savefig('class_report3.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7dc2453",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Confusion Matrix\n",
    "conf_matrix = (confusion_matrix(validation_generator.classes, y_pred_labels))\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb12cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, ax = plt.subplots(figsize=(15,12)) \n",
    "ww = sns.heatmap(conf_matrix, annot=True, xticklabels= (train_generator.class_indices.keys()), yticklabels= (train_generator.class_indices.keys()));\n",
    "ww.set_xlabel('True Class', fontsize=25)\n",
    "ww.set_ylabel('Predicted Class', fontsize=25)\n",
    "\n",
    "# Export plot\n",
    "plt.savefig('conf_matrix3.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797a6f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "class_dictionary = train_generator.class_indices\n",
    "class_dictionary = {\n",
    "    value:key for key, value in class_dictionary.items()\n",
    "}\n",
    "print(class_dictionary)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a6ec3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the class dictionary to pickle\n",
    "face_label_filename = 'face-labels.pickle'\n",
    "with open(face_label_filename, 'wb') as f: pickle.dump(class_dictionary, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866fd452",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates a HDF5 file\n",
    "pmodel.save(\n",
    "    'transfer_learning_facial_recognition' +\n",
    "    '_face_cnn_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19c21f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# deletes the existing model\n",
    "del pmodel\n",
    "\n",
    "# returns a compiled model identical to the previous one\n",
    "pmodel = load_model(\n",
    "    'transfer_learning_facial_recognition' +\n",
    "    '_face_cnn_model.h5')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01fd7f7a",
   "metadata": {},
   "source": [
    "# Running Facial Recognition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c3edb886",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'Gabby', 1: 'Gloria', 2: 'Guy', 3: 'Lolomari', 4: 'Nike', 5: 'man', 6: 'ola', 7: 'quo'}\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 306ms/step\n",
      "1/1 [==============================] - 0s 268ms/step\n",
      "1/1 [==============================] - 0s 448ms/step\n",
      "1/1 [==============================] - 0s 265ms/step\n",
      "1/1 [==============================] - 0s 258ms/step\n",
      "1/1 [==============================] - 0s 267ms/step\n",
      "1/1 [==============================] - 0s 260ms/step\n",
      "1/1 [==============================] - 0s 282ms/step\n",
      "1/1 [==============================] - 0s 265ms/step\n",
      "1/1 [==============================] - 0s 259ms/step\n",
      "1/1 [==============================] - 0s 282ms/step\n",
      "1/1 [==============================] - 0s 277ms/step\n",
      "1/1 [==============================] - 0s 267ms/step\n",
      "1/1 [==============================] - 0s 256ms/step\n",
      "1/1 [==============================] - 0s 263ms/step\n",
      "1/1 [==============================] - 0s 277ms/step\n",
      "1/1 [==============================] - 0s 269ms/step\n",
      "1/1 [==============================] - 0s 286ms/step\n",
      "1/1 [==============================] - 0s 272ms/step\n",
      "1/1 [==============================] - 0s 258ms/step\n",
      "1/1 [==============================] - 0s 278ms/step\n",
      "1/1 [==============================] - 0s 249ms/step\n",
      "1/1 [==============================] - 0s 262ms/step\n",
      "1/1 [==============================] - 0s 249ms/step\n",
      "1/1 [==============================] - 0s 258ms/step\n",
      "1/1 [==============================] - 0s 262ms/step\n",
      "1/1 [==============================] - 0s 255ms/step\n",
      "1/1 [==============================] - 0s 269ms/step\n",
      "1/1 [==============================] - 0s 254ms/step\n",
      "1/1 [==============================] - 0s 275ms/step\n",
      "1/1 [==============================] - 0s 254ms/step\n",
      "1/1 [==============================] - 0s 263ms/step\n",
      "1/1 [==============================] - 0s 257ms/step\n",
      "1/1 [==============================] - 0s 272ms/step\n",
      "1/1 [==============================] - 0s 280ms/step\n",
      "1/1 [==============================] - 0s 263ms/step\n",
      "1/1 [==============================] - 0s 268ms/step\n",
      "1/1 [==============================] - 0s 269ms/step\n",
      "1/1 [==============================] - 0s 276ms/step\n",
      "1/1 [==============================] - 0s 270ms/step\n",
      "1/1 [==============================] - 0s 280ms/step\n",
      "1/1 [==============================] - 0s 276ms/step\n",
      "1/1 [==============================] - 0s 264ms/step\n",
      "1/1 [==============================] - 0s 264ms/step\n",
      "1/1 [==============================] - 0s 259ms/step\n",
      "1/1 [==============================] - 0s 261ms/step\n",
      "1/1 [==============================] - 0s 265ms/step\n",
      "1/1 [==============================] - 0s 267ms/step\n",
      "1/1 [==============================] - 0s 276ms/step\n",
      "1/1 [==============================] - 0s 276ms/step\n",
      "1/1 [==============================] - 0s 257ms/step\n",
      "1/1 [==============================] - 0s 244ms/step\n",
      "1/1 [==============================] - 0s 253ms/step\n",
      "1/1 [==============================] - 0s 268ms/step\n",
      "1/1 [==============================] - 0s 265ms/step\n",
      "1/1 [==============================] - 0s 274ms/step\n",
      "1/1 [==============================] - 0s 252ms/step\n",
      "1/1 [==============================] - 0s 251ms/step\n",
      "1/1 [==============================] - 0s 260ms/step\n",
      "1/1 [==============================] - 0s 259ms/step\n",
      "1/1 [==============================] - 0s 251ms/step\n",
      "1/1 [==============================] - 0s 252ms/step\n",
      "1/1 [==============================] - 0s 252ms/step\n",
      "1/1 [==============================] - 0s 267ms/step\n",
      "1/1 [==============================] - 0s 259ms/step\n",
      "1/1 [==============================] - 0s 256ms/step\n",
      "1/1 [==============================] - 0s 252ms/step\n",
      "1/1 [==============================] - 0s 254ms/step\n",
      "1/1 [==============================] - 0s 264ms/step\n",
      "1/1 [==============================] - 0s 255ms/step\n",
      "1/1 [==============================] - 0s 256ms/step\n",
      "1/1 [==============================] - 0s 247ms/step\n",
      "1/1 [==============================] - 0s 257ms/step\n",
      "1/1 [==============================] - 0s 270ms/step\n",
      "1/1 [==============================] - 0s 251ms/step\n",
      "1/1 [==============================] - 0s 259ms/step\n",
      "1/1 [==============================] - 0s 276ms/step\n",
      "1/1 [==============================] - 0s 245ms/step\n",
      "1/1 [==============================] - 0s 291ms/step\n",
      "1/1 [==============================] - 0s 254ms/step\n",
      "1/1 [==============================] - 0s 253ms/step\n",
      "1/1 [==============================] - 0s 273ms/step\n",
      "1/1 [==============================] - 0s 260ms/step\n",
      "1/1 [==============================] - 0s 266ms/step\n",
      "1/1 [==============================] - 0s 260ms/step\n",
      "1/1 [==============================] - 0s 248ms/step\n",
      "1/1 [==============================] - 0s 256ms/step\n",
      "1/1 [==============================] - 0s 253ms/step\n",
      "1/1 [==============================] - 0s 260ms/step\n",
      "1/1 [==============================] - 0s 249ms/step\n",
      "1/1 [==============================] - 0s 269ms/step\n",
      "1/1 [==============================] - 0s 272ms/step\n",
      "1/1 [==============================] - 0s 293ms/step\n",
      "1/1 [==============================] - 0s 292ms/step\n",
      "1/1 [==============================] - 0s 258ms/step\n",
      "1/1 [==============================] - 0s 260ms/step\n",
      "1/1 [==============================] - 0s 263ms/step\n",
      "1/1 [==============================] - 0s 255ms/step\n",
      "1/1 [==============================] - 0s 272ms/step\n",
      "1/1 [==============================] - 0s 243ms/step\n",
      "1/1 [==============================] - 0s 249ms/step\n",
      "1/1 [==============================] - 0s 267ms/step\n",
      "1/1 [==============================] - 0s 271ms/step\n",
      "1/1 [==============================] - 0s 251ms/step\n",
      "1/1 [==============================] - 0s 254ms/step\n",
      "1/1 [==============================] - 0s 258ms/step\n",
      "1/1 [==============================] - 0s 260ms/step\n",
      "1/1 [==============================] - 0s 276ms/step\n",
      "1/1 [==============================] - 0s 257ms/step\n",
      "1/1 [==============================] - 0s 254ms/step\n",
      "1/1 [==============================] - 0s 271ms/step\n",
      "1/1 [==============================] - 0s 254ms/step\n",
      "1/1 [==============================] - 0s 266ms/step\n",
      "1/1 [==============================] - 0s 261ms/step\n",
      "1/1 [==============================] - 0s 249ms/step\n",
      "1/1 [==============================] - 0s 263ms/step\n",
      "1/1 [==============================] - 0s 269ms/step\n",
      "1/1 [==============================] - 0s 261ms/step\n",
      "1/1 [==============================] - 0s 261ms/step\n",
      "1/1 [==============================] - 0s 263ms/step\n",
      "1/1 [==============================] - 0s 262ms/step\n",
      "1/1 [==============================] - 0s 254ms/step\n",
      "1/1 [==============================] - 0s 266ms/step\n",
      "1/1 [==============================] - 0s 264ms/step\n",
      "1/1 [==============================] - 0s 262ms/step\n",
      "1/1 [==============================] - 0s 258ms/step\n",
      "1/1 [==============================] - 0s 276ms/step\n",
      "1/1 [==============================] - 0s 252ms/step\n",
      "1/1 [==============================] - 0s 252ms/step\n",
      "1/1 [==============================] - 0s 263ms/step\n",
      "1/1 [==============================] - 0s 254ms/step\n",
      "1/1 [==============================] - 0s 250ms/step\n",
      "1/1 [==============================] - 0s 263ms/step\n",
      "1/1 [==============================] - 0s 267ms/step\n",
      "1/1 [==============================] - 0s 250ms/step\n",
      "1/1 [==============================] - 0s 243ms/step\n",
      "1/1 [==============================] - 0s 248ms/step\n",
      "1/1 [==============================] - 0s 248ms/step\n",
      "1/1 [==============================] - 0s 268ms/step\n",
      "1/1 [==============================] - 0s 253ms/step\n",
      "1/1 [==============================] - 0s 242ms/step\n",
      "1/1 [==============================] - 0s 251ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import cv2\n",
    "import pickle\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "count = 0\n",
    "import os\n",
    "\n",
    "# for face detection\n",
    "modelFile = \"dnn_config/res10_300x300_ssd_iter_140000.caffemodel\"\n",
    "configFile = \"dnn_config/deploy.prototxt.txt\"\n",
    "net = cv2.dnn.readNetFromCaffe(configFile, modelFile)\n",
    "\n",
    "\n",
    "# resolution of the webcam\n",
    "screen_width = 1280       # try 640 if code fails\n",
    "screen_height = 720\n",
    "\n",
    "# size of the image to predict\n",
    "image_width = 224\n",
    "image_height = 224\n",
    "\n",
    "# load the trained model\n",
    "model = load_model('transfer_learning_facial_recognition_face_cnn_model.h5')\n",
    "\n",
    "\n",
    "# the labels for the trained model\n",
    "with open(\"face-labels.pickle\", 'rb') as f:\n",
    "    og_labels = pickle.load(f)\n",
    "    labels = {key:value for key,value in og_labels.items()}\n",
    "    print(labels)\n",
    "\n",
    "# default webcam\n",
    "stream = cv2.VideoCapture(0)\n",
    "stream.set(3, 1280) # set video width\n",
    "stream.set(4, 720) # set video height\n",
    "# Define min window size to be recognized as a face\n",
    "minW = 0.1*stream.get(3)\n",
    "minH = 0.1*stream.get(4)\n",
    "\n",
    "while(True):\n",
    "    ret, img = stream.read()\n",
    "    (grabbed, frame) = stream.read()\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    \n",
    "    if grabbed == True:\n",
    "        frame = cv2.resize(frame, None, fx=0.5, fy=0.5)\n",
    "        height, width = frame.shape[:2]\n",
    "        \n",
    "        rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        blob = cv2.dnn.blobFromImage(cv2.resize(frame, (300,300)),\n",
    "                                        1.0, (300,300), (104.0, 117.0, 123.0))\n",
    "        net.setInput(blob)\n",
    "        faces3 = net.forward()\n",
    "        \n",
    "        for i in range(faces3.shape[2]):\n",
    "            confidence = faces3[0, 0, i, 2]\n",
    "            if confidence > 0.5:\n",
    "                box = faces3[0, 0, i, 3:7] * np.array([width, height, width, height])\n",
    "                (x, y, x1, y1) = box.astype(\"int\")\n",
    "                cv2.rectangle(frame, (x, y), (x1, y1), (0, 0, 255), 2)\n",
    "                count += 1\n",
    "\n",
    "\n",
    "        # predict the image\n",
    "        frame1 = cv2.resize(frame, (224, 224))\n",
    "        frame1 = frame1.reshape(1,224,224,3)\n",
    "        predicted_prob = model.predict(frame1)\n",
    "\n",
    "        # Display the label\n",
    "        font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "        name = labels[predicted_prob[0].argmax()]\n",
    "        color = (255, 0, 255)\n",
    "        stroke = 2\n",
    "        cv2.putText(frame, f'({name})', (x,y-8),\n",
    "            font, 1, color, stroke, cv2.LINE_AA)\n",
    "        \n",
    "\n",
    "        # Show the frame\n",
    "        cv2.imshow(\"Image\", frame)\n",
    "        key = cv2.waitKey(1) & 0xFF\n",
    "        if key == ord(\"q\"):    # Press q to break out of the loop\n",
    "            break      \n",
    "\n",
    "# Cleanup\n",
    "stream.release()\n",
    "cv2.waitKey(1)\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8f20a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "frame.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cafcc7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "height\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf35131b",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_prob"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
